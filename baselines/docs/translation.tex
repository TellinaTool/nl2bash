%!TEX root=writeup.tex
\subsection{Natural Language to Bash Command Translation}
\label{subsec:parser}

We adopted a top-down approach that searches for the most relevant bash commands for a given natural language description. Specifically, we designed an efficient greedy search procedure (\S~\ref{subsec:decoding}) that enumerates the set of relevant bash commands generated by the bash grammar presented in \autoref{fig:grammars}, and extracts a set of features for each of them paired with the natural language description (\S~\ref{subsec:feature}). The set of candidate commands are then ranked by a linear scoring function, the weights of which are learned from a training set (\S~\ref{subsec:training}).

\subsubsection{Decoding}
\label{subsec:decoding}

To control scalability, we consider only the following small set of head commands: \texttt{find, mv, sort, grep, cp, ls, tar, xargs}. Nevertheless, the number of possible bash commands covered by this set is still daunting. Take the \texttt{find} command as an example, it has nearly 40 options and hence $2^{40}$ bash command candidates\footnote{Most of the command-option combinations are pretty rare. However, we haven't found an effective way to guide search with this information.}. To prevent the search space from blowing up, we consider only candidate commands that contain $2-5$ terms (a ``term'' is either a head command, a flag, or an argument). Furthermore, we use a greedy search procedure which only explores the top-$k$ best children of each node based on the scoring of the partial command ending at the child node. We designed the features to be factorized over each term of the program, i.e. a set of features are extracted for each term in the command candidate and the final feature set is the union of them. This allows features of a bigger program structure to be computed based on the features of its substructures. While this simplified feature set misses most information regarding both the structure of the commands and the structure of the language, we found it to frequently capture the gist of a translation in practice.

The greedy procedure for bash command enumeration and feature extraction is summarized in alg.~\ref{alg:decoding}.

\begin{algorithm} [ht]
\caption{Bash command enumeration and feature extraction\label{alg:decoding}}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKwInOut{Initial}{Initialization}
\SetKwFunction{Add}{AddTo}
\SetKwFunction{DFS}{DFS}
\SetKwFunction{ExtractFeat}{ExtractFeatures}
\SetKwFunction{TopKBest}{TopKBestNextTerms}
\SetKwFunction{EndOfCmd}{IsEndOfCmd}
\SetKwFunction{PrintCmd}{GetCmdAndFeaturesWithBacktracking}

\Input{nl\_cmd, HeadCmdSet}
\Output{CmdsAndFeatures}
\Initial{CmdsAndFeatures=\{\}}
\For {head\_cmd $\in$ HeadCmdSet}{
	\DFS{head\_cmd, nl\_cmd}
}
\SetKwProg{fun}{Procedure}{}{}
\fun{\DFS{term, nl\_cmd}}{
	term.features = \ExtractFeat{term, nl\_cmd} \\
	\If {\EndOfCmd{term}} {
		CmdsAndFeatures $\leftarrow$ CmdsAndFeatures + \\
			\PrintCmd{term}
	}
	\For {next\_term $\in$ \TopKBest{term}} {
		next\_term.backpointer = term \\
		\DFS{next\_term, nl\_cmd}
	}
}
\fun{\ExtractFeat{term, nl\_cmd}}{
	FeatureSet = $\emptyset$

	\For {word $\in$ nl\_cmd} {
		FeatureSet = FeatureSet + (term, word)
	}
	\KwRet{FeatureSet}
}
\end{algorithm}

\subsubsection{Features}
\label{subsec:feature}
% The following features are used:
% \begin{itemize}\itemsep-1pt
%	\item association of key words/phrases to partial expressions
%  	\item association between partial expressions (e.g. how often do they combined in valid commands)
%	\item similarity of key words/phrases in the command to the man page explanation text of a partial expression
%	\item complexity of the logical formulas and the commands generated from them.
% \end{itemize}
As shown in the bottom function in alg.~\ref{alg:decoding}, the feature set consists of simple term-word tuples, where a \emph{term} is a token found in Bash source code and a \emph{word} is an English word in the natural language. When a term appears in the input that was not present in the training data, we replace it with the special ``unk\_term'' symbol. Similarly, and any unseen words become the ``unk\_word'' symbol. To discriminate important features from unimportant ones, we assigned each tuple its PMI value, which is computed on the training set using the following equations:

\begin{align}
	\|PMI|(t, w) &= \log{\frac{P^2(t,w)}{P(t)P(w)}} \\
	P(t, w) &= \frac{\|count\_of\_|(t,w)}{\|total\_count\_of\_tuples|} \\
	P(t) &= \frac{\|count\_of\_t|}{\|total\_count\_of\_terms|} \\
	P(w) &= \frac{\|count\_of\_w|}{\|total\_count\_of\_words|}.
\end{align}

The PMI value measures the association between a pair of term and word based on the training set. It has the effect of down-weighting uninformative tuples such as ``(\texttt{find}, and)''---both \<find> and ``and'' are very common tokens. However, we also noticed some anomalies associated with this approach. For example, the tuple ``(\texttt{-f}, file)'' has a very low PMI value, although the \texttt{-f} option generally refers to a file. This is because the word ``file'' occurs in almost every training example in our dataset (with and without \texttt{-f} present). Therefore simple example-level occurrence measure wasn't able to reveal the connection between \texttt{-f} and ``file''. We suspect that statistical alignment, which infers the probability of the alignment between each term and word in every example, would work better than exhaustive pair-up. The design of an alignment algorithm is left to future work.

\subsubsection{Training data}

Our training data comes from question-answer pairs from StackOverflow%
\footnote{\url{https://archive.org/details/stackexchange}, retrieved Jan. 21,
2016}. In the initial dataset there are 42111 questions having accepted answers
tagged with one of the
tags ``bash,'' ``sh,'' ``shell,'' ``zsh,'' ``command-line,'' ``awk,'' ``sed,''
``xargs,'' ``tar,'' ``rsync,'' ``scp,'' ``mv,'' or ``cp''. From the accepted
answer of each question we extract every code snippet and pair it with the
question title, resulting in zero or more (title, code) pairs. In total there
are 75518 such pairs. Of these, only 29880 parse as legal bash code (after
removing comments and fake ``\$'' prompts at the beginnings of lines).

Unfortunately, the StackOverflow data are very noisy: some of the questions are not well described and the expert obtained the questioner's intent though conversations. As a result, the text/command pairs obtained are of low quality (not well corresponded). On the other hand, a relatively small amount of clean data can be obtained by manually collecting from tutorials or forums. These data are of relatively high quality, but we were only able to collect roughly 200 such examples.

We opted to use the StackOverflow dataset, as we believed that a larger dataset
could compensate for the noise. To combat the problem we also performed
additional cleaning:
\begin{itemize}
	\item Removed punctuation, stop words, and stop phrases (such as ``bash'',
		``how'', and ``in command line''). The English words ``not'' and ``no''%
		---which are traditionally used as stop words in information retrieval%
		---are \emph{not} removed during this step, as have relevant meaning.
	\item Lemmatized words using the Stanford CoreNLP library~%
		\footnote{\url{https://stanfordnlp.github.io/CoreNLP/}}
	\item Bash code spanning more than one line is ignored (this step removes
		some pairs from the training data)
	\item Bash redirect removal; e.g., \code{cat >/dev/null} becomes simply \code{cat}
\end{itemize}

\autoref{tbl:stats} shows a few simple statistics about the training data.
Training the entire model takes several hours due to the fact that evaluating a
single input takes roughly 15 seconds.

\begin{table}
    \begin{center}
    \begin{tabular}[t]{lr}
        \hline
        \# of questions                & 42111 \\
        \# of (question, answer) pairs & 75518 \\
        \# of parseable pairs          & 29880 \\
        \# of clean pairs              & 20032 \\
        \hline
    \end{tabular}
    \end{center}
    \caption{Training data statistics}
    \label{tbl:stats}
\end{table}

\subsubsection{Training}
\label{subsec:training}

We use the structured perception algorithm (alg.~\ref{alg:training}) to learn weights of the scoring function from the example pairs. In the training process, the input are pairs collected from StackOverflow, and the target is to learn a scoring functions to evaluate the correspondence between the a natural language description and a bash command. In each training iteration, top ranked logical form are selected and the weights are updated based on its similarity to the ground truth logical form.

\begin{algorithm}
\caption{Perceptron Training\label{alg:training}}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKwFunction{Feat}{FeatureVectorOf}
\SetKwFunction{TopPred}{TopPrediction}
\Input{$\{(bash\_cmd_i,nl\_cmd_i)|i=1\hdots N\}$}
\Output{weights}
\For{$t = 1,\hdots, T$}{
	\For{$i=1\hdots N$} {
		prediction = \TopPred{$nl\_cmd_i$}

		\If {prediction != $bash\_cmd_i$} {
			weights = weights + 2 * \Feat{$bash\_cmd_i$} - \Feat{prediction}
		}
	}
}
\end{algorithm}
