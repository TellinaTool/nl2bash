%!TEX root=writeup.tex
\section{Related Work}

% \paragraph{Domain-Specific Natural Language Programming} The problem of translating natural language into executable code have been studied since decades ago~\cite{Ballard:1979:PNL:800177.810072,Pedersen-Report}. Most of previous research prototyped on domain-specific languages, ranging from the Structured Query Language (SQL)~\cite{Nihalani_NLIDB_review} to text-editing commands for Office suite applications~\cite{DBLP:journals/corr/DesaiGHJKMRR15}.

\paragraph{Programming by Natural Language} Programming by natural language (PBNL) is a technique to translate natural language descriptions to structured programs. Several PBNL systems exist to support database queries~\cite{DBLP:conf/sigmod/GulwaniM14, DBLP:journals/tods/LiYJ07, DBLP:journals/pvldb/LiJ14}, text editing~\cite{DBLP:journals/corr/DesaiGHJKMRR15}, API call synthesis~\cite{DBLP:journals/corr/RaghothamanWH15} and smartphone scripts synthesis~\cite{DBLP:conf/mobisys/LeGS13}.

NaLIR~\cite{DBLP:journals/pvldb/LiJ14} is a system designed to construct SQL queries from natural language. NaLIR operates by first parsing a natural language sentence into a parse tree and then converting the parse tree to a SQL query according to hand-coded rules. The system is able to resolve possible ambiguities by asking yes/no questions to users during the conversion. Since NaLIR requires a well-formed parse tree to infer the mapping, it depends on relatively high natural language qualities. Unlike NaLIR, our system learns the translation rules using training data and manpages. We will rely on users to fix any ambiguities on their own after the tool runs instead of by asking explicit questions during operation.

Desai et al.~\cite{DBLP:journals/corr/DesaiGHJKMRR15} provide a general framework PBNL systems and demonstrate its success in several domains including text-editing and QA system answering. This framework is generalized from NLyze~\cite{DBLP:conf/sigmod/GulwaniM14} and SmartSynth~\cite{DBLP:conf/mobisys/LeGS13} and provides a meta language to design PBNL systems for new domains. The framework requires a set of basic mapping rules as input. It first generates a set of program fragments based on the NL specification, and then enumerates all possible programs consisting of these fragments. A scoring scheme ranks the programs based on feature coverage and structural similarity. Command-line instructions do not fit neatly into this framework since they have simple structure but many basic expressions. Again, our system avoids manually constructing rewrite rules through machine learning. A benefit of this approach is that the range of possible outputs can be extended very easily---potentially on-the-fly based on what programs are available on the user's machine.

\paragraph{Semantic Parsing} 
% A number of natural language processing (NLP) research were conducted with a special focus on molding natural language into programming tools~\cite{mihalcea2006nlp,LandhauBer:2015:TUP:2820668.2820671}. Besides, 
%the task of synthesizing programs from natural language meets the 
Semantic parsing (SP) refers to the problem of parsing natural language into logical forms. This area of research is closely related to PBNL although it was mostly conducted in the natural language processing (NLP) community. Compared to PBNL, the focus of which is generating executable programs, semantic parsing focuses on modeling the ``meaning'' of the natural language utterance. Since developing a universal and precise meaning representation for natural language is extremely difficult, the output of SP systems ranges from executable code~\cite{conf/iser/MatuszekHZF12} to purely linguistics-motivated meaning representations~\cite{lewis2015joint}.

Compared to works in PBNL, state-of-the-art SP systems emphasize the modeling of natural language structures, such as phrasal structure and syntactic dependencies. The syntactic and lexical information are then furthur abstracted into either a special-purpose~\cite{DBLP:conf/naacl/KushmanB13,DBLP:conf/acl/LeiLBR13} or broad-coverage~\cite{kwiatkowski-EtAl:2013:EMNLP} meaning representation, which either forms the final output of the system or is converted to executable programs, very often deterministically~\cite{DBLP:conf/naacl/KushmanB13,DBLP:conf/acl/LeiLBR13}.

Due to the close relationship between statistical learning and NLP, most of the state-of-the-art SP systems are data-driven~\cite{Zettlemoyer05learningto,Kwiatkowski:2011:LGC:2145432.2145593,DBLP:Poon13}. Due to the fact that manually labeling a large amount of text with its meaning representation is laborious and error-prone, as well as that different meaning representations often succeeds in different tasks, state-of-the-art SP systems are often trained with feedbacks from downstream tasks, while keeping the meaning representation latent~\cite{Artzi:2011:BSP:2145432.2145481,kwiatkowski-EtAl:2013:EMNLP}. ~\cite{Artzi:2011:BSP:2145432.2145481} learn to map from natural language sentences to lambda-expressions using conversational interaction without any explicitly annotated logical forms.
~\cite{kwiatkowski-EtAl:2013:EMNLP} learns to map natural language questions to executable queries on a given ontology, using question-answer pairs as the supervision and a linguistically motivated logical form as an intermediate representation.
~\cite{conf/iser/MatuszekHZF12} learns to parse natural language commands into action and control structures that can be executed in a robotics system, based on pairs of English commands and control structures, situated in the context of robots following routing instructions through an indoor environment.

The method we tried for the course project falls more into the PBNL category rather than SP. Since we are not applying sophisticated language structure modeling. We dropped language structure modeling because of two reasons. First, most of the NLP tools are trained for processing written texts hence have poor accuracy in processing the NL commands. Second, we hope the resulting system could be more robust to non-grammatical input from the users\footnote{We are still not sure how often this would happen. A user study might be helpful}. It turned out that a certain degree of language structure modeling is necessary, and we probably need to do some task-specific development as~\cite{DBLP:conf/emnlp/KiddonPZC15} did. We discuss this in \S~\ref{future:semantics}.

% There has been continuous interest in formally modeling the meaning of natural language since the dawn of the NLP field~\cite{opac-b1080356}. Early approaches relying on the linguistic knowledge of human experts, however, were proved difficult to scale and result in logical representations that were too brittle to be used in downstream applications. 
% Recently there has been a surge of interest from the NLP community in learning to map natural language into formal meaning representations~\cite{Zettlemoyer05learningto,Kwiatkowski:2011:LGC:2145432.2145593,DBLP:Poon13}, due to the progress in statistical methods, corpora annotation and learning models that leverages feedback from downstream applications~\cite{Artzi:2011:BSP:2145432.2145481,kwiatkowski-EtAl:2013:EMNLP,conf/iser/MatuszekHZF12}. The task was broadly named semantic parsing. Moreover, improvement of performance has been shown when the syntax and semantics of natural language are jointly modeled~\cite{Zettlemoyer05learningto,lewis2015joint}.
% ~\cite{Zettlemoyer05learningto} induces a grammar that maps natural language sentences to lambda-calculus encoding of their meanings, along with a log-linear probabilistic model of the joint distribution over syntactic and semantic representations conditioned on the input sentence. They trained the model based on annotated natural language sentence and lambda-calculus representation pairs.


% In the programming by natural language case, semantic parsing is a natural intermediate step, where it maps natural language commands into the formal specification that program synthesis relies on. Compared to previous works, our approach is less dependent on syntactic modeling and attempts to learn a mapping directly from commands to the intermediate logic representation\footnote{This might change as the model develops.}. We choose to use only shallow syntactic features (e.g. part-of-speech taggings) in the hope that the system could accept not only the grammatically well-formed commands, but also commands that contains broken natural language and even mixed with partial command line expressions. We use a small amount of example natural language and command line pairs as one source of supervision. Another source of our supervision comes from the Linux man-page, where the examples are not exactly like commands initiated by the users, but contains rich natural language documentation of each command template and its possible arguments. Since acquiring sufficient number of NL to command line examples is prohibitive, we expect the parser to be largely improved if the model could leverage the natural language explanation of each command component.
% Recent advances in natural language processing have enabled the adoption of more sophisticated linguistic representation, such as the combinatory categorical grammar (CCG)~\cite{opac-b1080082}, as features for constructing the mapping. It also sheds light on more efficient parsing~\cite{lewis2015joint} and more effective machine learning algorithms~\cite{DBLP:journals/corr/ZarembaS14} which makes more efficient and precise NL-based program synthesis feasible.
% Compared to traditional syntactic parsing that maps natural language to its syntactic parse tree~\cite{Klein:2003:AUP:1075096.1075150}, semantic parsing. 
% Despite the development of linguistic-based semantic theory~\cite{dowty1989} and abstract meaning representation~\cite{Banarescu13abstractmeaning}, the majority of the semantic parsing frameworks were implanted in real-world applications

\paragraph{Existing Natural Language Based Command Line Helper Tools} When implementing our system, we discovered a few interesting command line helper tools on Github, some of which are highly rated among the community. HowDoI\footnote{\url{https://github.com/gleitz/howdoi}} and Betty\footnote{\url{https://github.com/pickhardt/betty}} are key-word driven rule-based systems which could prompt users a limited set of command usage via natural language conversations. Explainshell\footnote{\url{https://github.com/idank/explainshell}} works in the opposite way that if users type in a bash command, it will explain the usage of each terms by simply showing its man page documentation.

\paragraph{Programming by Demonstration} Instead of asking for natural language input, programming by demonstration (PBD) systems~\cite{DBLP:journals/ml/LauWDW03, DBLP:journals/cacm/GulwaniHS12, DBLP:conf/pldi/HarrisG11, DBLP:conf/popl/Gulwani11} ask the user to specify the task by providing input/output examples or demonstrating operation traces. However, since command-line operations may involve many different files they can be difficult to write examples or demonstrated by the user in practice.

\paragraph{Keyword Programming} Keyword programming techniques synthesize programs (or program fragments) by heuristically searching a large software corpus for programs that are similar to the given keywords. Keyword programming has shown some success in specific tasks like API call generation~\cite{DBLP:journals/ase/LittleM09, DBLP:conf/pldi/MandelinXBK05} and database queries~\cite{DBLP:conf/icde/AgrawalCD02, DBLP:conf/icde/BhalotiaHNCS02}. However, neither structural information nor correlations of keywords in the users description is captured by the keyword based search approach, keyword programming results in relatively low precision and often fails to recommend correct programs in text to command task.